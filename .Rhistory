# (a) Find the probability that the user spends more than 15 minutes per month at the site.
round(pnorm(15, mean = 25, sd = 4, lower.tail = FALSE), 4) #P(X<15) right tail
# (b) Find the probability that the user spends between 20 and 35 minutes per month at the site.
#P(20<=X<=35)
round(pnorm(35, mean=25, sd=4) - pnorm(20, mean =25, sd=4),4)
# (c) What is the amount of time per month a user spends on Facebook, if only 1% of users spend this time or longer on Facebook?
#P(X>x) = 0.01 & P(X<x) = 0.99
#looking for 99th percentile
round(qnorm(0.99, mean=25, sd=4),4)
# (d) Between what values do the time spent of the middle 90% distribution of Facebook users fall?
#P(0.05<X<0.95)
round(qnorm(0.95, mean=25, sd=4),4)
round(qnorm(0.05, mean=25, sd=4),4)
# (a) Find the probability that the user spends more than 15 minutes per month at the site.
round(pnorm(15, mean = 25, sd = 4, lower.tail = FALSE), 4) #P(X<15) right tail
round(pnorm(15, mean = 25, sd = 4, lower.tail = FALSE), 4)
# Output: 0.9938 ✅
# (b) Find the probability that the user spends between 20 and 35 minutes per month at the site.
#P(20<=X<=35)
round(pnorm(35, mean=25, sd=4) - pnorm(20, mean =25, sd=4),4)
# (c) What is the amount of time per month a user spends on Facebook, if only 1% of users spend this time or longer on Facebook?
#P(X>x) = 0.01 & P(X<x) = 0.99
#looking for 99th percentile
round(qnorm(0.99, mean=25, sd=4),4)
# (d) Between what values do the time spent of the middle 90% distribution of Facebook users fall?
#P(0.05<X<0.95)
round(qnorm(0.95, mean=25, sd=4),4)
round(qnorm(0.05, mean=25, sd=4),4)
# (a) Find the probability that the user spends more than 15 minutes per month at the site.
round(pnorm(15, mean = 25, sd = 4, lower.tail = FALSE), 4) #P(X<15) right tail
round(pnorm(15, mean = 25, sd = 4, lower.tail = FALSE), 4)
# Output: 0.9938 ✅
# (b) Find the probability that the user spends between 20 and 35 minutes per month at the site.
#P(20<=X<=35)
round(pnorm(35, mean=25, sd=4) - pnorm(20, mean =25, sd=4),4)
round(pnorm(35, mean = 25, sd = 4) - pnorm(20, mean = 25, sd = 4), 4)
# Output: 0.8881 ✅
# (c) What is the amount of time per month a user spends on Facebook, if only 1% of users spend this time or longer on Facebook?
#P(X>x) = 0.01 & P(X<x) = 0.99
#looking for 99th percentile
round(qnorm(0.99, mean=25, sd=4),4)
# (d) Between what values do the time spent of the middle 90% distribution of Facebook users fall?
#P(0.05<X<0.95)
round(qnorm(0.95, mean=25, sd=4),4)
round(qnorm(0.05, mean=25, sd=4),4)
# (a) Find the probability that the user spends more than 15 minutes per month at the site.
round(pnorm(15, mean = 25, sd = 4, lower.tail = FALSE), 4) #P(X<15) right tail
round(pnorm(15, mean = 25, sd = 4, lower.tail = FALSE), 4)
# Output: 0.9938 ✅
# (b) Find the probability that the user spends between 20 and 35 minutes per month at the site.
#P(20<=X<=35)
round(pnorm(35, mean=25, sd=4) - pnorm(20, mean =25, sd=4),4)
round(pnorm(35, mean = 25, sd = 4) - pnorm(20, mean = 25, sd = 4), 4)
# Output: 0.8881 ✅
# (c) What is the amount of time per month a user spends on Facebook, if only 1% of users spend this time or longer on Facebook?
#P(X>x) = 0.01 & P(X<x) = 0.99
#looking for 99th percentile
round(qnorm(0.99, mean=25, sd=4),4)
round(qnorm(0.99, mean = 25, sd = 4), 4)
# Output: 34.3054 ✅
# (d) Between what values do the time spent of the middle 90% distribution of Facebook users fall?
#P(0.05<X<0.95)
round(qnorm(0.95, mean=25, sd=4),4)
round(qnorm(0.05, mean=25, sd=4),4)
knitr::opts_chunk$set(echo = TRUE)
pnorm(15, mean=25, sd=4, lower.tail = FALSE)
round(pnorm(15, mean=25, sd=4, lower.tail = FALSE),4)
round(pnorm(35, mean=25, sd=4) - pnorm(20, mean=25, sd=4),4)
round(qnorm(0.99, mean=25, sd=4),4)
round(qnorm(c(0.05, 0.95), mean=25, sd=4),4)
#No Disease Stat
mean_nd <-195.2745
var_nd <-1303.9231
n_nd <- 51
#Disease Stat
mean_d <- 216.1906
var_d <- 1850.2488
n_d <- 320
#Data Simulation from Summary Stats
group_nd <- rnorm(n_nd, mean_nd, sqrt(var_nd))
group_d <- rnorm(n_d, mean_d, sqrt(var_d))
#Two-sample t-test with var.equal=FALSE
t.test(group_nd, group_d, var.equal = FALSE)
t.test(group_nd, group_d, var.equal=FALSE, conf.level=0.99)
t.test(group_nd, group_d, var.equal=FALSE, conf.level=0.99)
glakes <- read.table("glakes.txt", header = TRUE)
model_M1 <- lm(Time ~ Tonnage, data = glakes)
plot(glakes$Tonnage, glakes$Time,
main = "Model M1: Time vs Tonnage",
xlab = "Tonnage", ylab = "Time", pch = 19, col = "blue")
abline(model_M1, col="red", lwd=2)
summary(model_M1)
par(mfrow = c(2,2))
plot(model_M1)
# Q3(c) - Outliers and Influential Points
influence_measures <- influence.measures(model_M1)
# Standardised residuals
std_resid <- rstandard(model_M1)
# Leverage
leverage <- hatvalues(model_M1)
# Cook's distance
cooks_d <- cooks.distance(model_M1)
# Combine into a dataframe
diagnostics <- data.frame(
Case = 1:nrow(glakes),
Tonnage = glakes$Tonnage,
Time = glakes$Time,
StdResiduals = round(std_resid, 3),
Leverage = round(leverage, 3),
CooksDistance = round(cooks_d, 3)
)
# Identify influential points
influential <- diagnostics[abs(diagnostics$StdResiduals) > 2 | diagnostics$CooksDistance > (4 / nrow(glakes)), ]
influential
# Q3(c) - Outliers and Influential Points
influence_measures <- influence.measures(model_M1)
# Standardised residuals
std_resid <- rstandard(model_M1)
# Leverage
leverage <- hatvalues(model_M1)
# Cook's distance
cooks_d <- cooks.distance(model_M1)
# Combine into a dataframe
diagnostics <- data.frame(
Case = 1:nrow(glakes),
Tonnage = glakes$Tonnage,
Time = glakes$Time,
StdResiduals = round(std_resid, 3),
Leverage = round(leverage, 3),
CooksDistance = round(cooks_d, 3)
)
# Identify influential points
influential <- diagnostics[abs(diagnostics$StdResiduals) > 2 | diagnostics$CooksDistance > (4 / nrow(glakes)), ]
influential
# Leverage vs Standardised Residuals
plot(leverage, std_resid,
main = "Leverage vs Standardised Residuals",
xlab = "Leverage", ylab = "Standardised Residuals",
pch = 19, col = "black")
abline(h = c(-2, 2), col = "red", lty = 2)
# Cook's Distance Plot
plot(cooks_d, type = "h",
main = "Cook's Distance",
xlab = "Observation", ylab = "Cook's Distance")
abline(h = 4 / nrow(glakes), col = "red", lty = 2)
# Q3(c) - Outliers and Influential Points
influence_measures <- influence.measures(model_M1)
# Standardised residuals
std_resid <- rstandard(model_M1)
# Leverage
leverage <- hatvalues(model_M1)
# Cook's distance
cooks_d <- cooks.distance(model_M1)
# Combine into a dataframe
diagnostics <- data.frame(
Case = 1:nrow(glakes),
Tonnage = glakes$Tonnage,
Time = glakes$Time,
StdResiduals = round(std_resid, 3),
Leverage = round(leverage, 3),
CooksDistance = round(cooks_d, 3)
)
# Identify influential points
influential <- diagnostics[abs(diagnostics$StdResiduals) > 2 | diagnostics$CooksDistance > (4 / nrow(glakes)), ]
influential
# Leverage vs Standardised Residuals
plot(leverage, std_resid,
main = "Leverage vs Standardised Residuals",
xlab = "Leverage", ylab = "Standardised Residuals",
pch = 19, col = "black")
abline(h = c(-2, 2), col = "red", lty = 2)
# Cook's Distance Plot
plot(cooks_d, type = "h",
main = "Cook's Distance",
xlab = "Observation", ylab = "Cook's Distance")
abline(h = 4 / nrow(glakes), col = "red", lty = 2)
# Q3(d) - Transform variables and fit model M2
glakes$logTime <- log(glakes$Time)
glakes$sqrtTonnage <- sqrt(glakes$Tonnage)
model_M2 <- lm(logTime ~ sqrtTonnage, data = glakes)
# Plot of transformed variables with fitted line
plot(glakes$sqrtTonnage, glakes$logTime,
main = "Model M2: log(Time) vs sqrt(Tonnage)",
xlab = "sqrt(Tonnage)", ylab = "log(Time)",
pch = 19, col = "darkgreen")
abline(model_M2, col = "red", lwd = 2)
# Q3(e) - Model summary and diagnostics for M2
summary(model_M2)
par(mfrow = c(2, 2))
plot(model_M2)
# Q3(g) - New data
newdata_m1 <- data.frame(Tonnage = 10000)
newdata_m2 <- data.frame(sqrtTonnage = sqrt(10000))
# Model M1 intervals
pred_m1 <- predict(model_M1, newdata = newdata_m1, interval = "prediction")
conf_m1 <- predict(model_M1, newdata = newdata_m1, interval = "confidence")
# Model M2 intervals (back-transform)
pred_m2_log <- predict(model_M2, newdata = newdata_m2, interval = "prediction")
conf_m2_log <- predict(model_M2, newdata = newdata_m2, interval = "confidence")
pred_m2 <- exp(pred_m2_log)
conf_m2 <- exp(conf_m2_log)
# Plot for Model M1
plot(glakes$Tonnage, glakes$Time,
main = "Model M1: Prediction & Confidence Intervals",
xlab = "Tonnage", ylab = "Time", pch = 19)
abline(model_M1, col = "blue", lwd = 2)
points(10000, pred_m1[1], col = "red", pch = 19)
arrows(10000, pred_m1[2], 10000, pred_m1[3], col = "red", angle = 90, code = 3, length = 0.1)
arrows(10000, conf_m1[2], 10000, conf_m1[3], col = "green", angle = 90, code = 3, length = 0.1)
# Plot for Model M2
plot(glakes$Tonnage, glakes$Time,
main = "Model M2: Prediction & Confidence Intervals",
xlab = "Tonnage", ylab = "Time", pch = 19)
curve(exp(predict(model_M2, newdata = data.frame(sqrtTonnage = sqrt(x)))),
add = TRUE, col = "blue", lwd = 2, from = min(glakes$Tonnage), to = max(glakes$Tonnage))
points(10000, pred_m2[1], col = "red", pch = 19)
arrows(10000, pred_m2[2], 10000, pred_m2[3], col = "red", angle = 90, code = 3, length = 0.1)
arrows(10000, conf_m2[2], 10000, conf_m2[3], col = "green", angle = 90, code = 3, length = 0.1)
# Q3(g) - New data
newdata_m1 <- data.frame(Tonnage = 10000)
newdata_m2 <- data.frame(sqrtTonnage = sqrt(10000))
# Model M1 intervals
pred_m1 <- predict(model_M1, newdata = newdata_m1, interval = "prediction")
conf_m1 <- predict(model_M1, newdata = newdata_m1, interval = "confidence")
# Model M2 intervals (back-transform)
pred_m2_log <- predict(model_M2, newdata = newdata_m2, interval = "prediction")
conf_m2_log <- predict(model_M2, newdata = newdata_m2, interval = "confidence")
pred_m2 <- exp(pred_m2_log)
conf_m2 <- exp(conf_m2_log)
# Plot for Model M1
plot(glakes$Tonnage, glakes$Time,
main = "Model M1: Prediction & Confidence Intervals",
xlab = "Tonnage", ylab = "Time", pch = 19)
abline(model_M1, col = "blue", lwd = 2)
points(10000, pred_m1[1], col = "red", pch = 19)
arrows(10000, pred_m1[2], 10000, pred_m1[3], col = "red", angle = 90, code = 3, length = 0.1)
arrows(10000, conf_m1[2], 10000, conf_m1[3], col = "green", angle = 90, code = 3, length = 0.1)
# Plot for Model M2
plot(glakes$Tonnage, glakes$Time,
main = "Model M2: Prediction & Confidence Intervals",
xlab = "Tonnage", ylab = "Time", pch = 19)
curve(exp(predict(model_M2, newdata = data.frame(sqrtTonnage = sqrt(x)))),
add = TRUE, col = "blue", lwd = 2, from = min(glakes$Tonnage), to = max(glakes$Tonnage))
points(10000, pred_m2[1], col = "red", pch = 19)
arrows(10000, pred_m2[2], 10000, pred_m2[3], col = "red", angle = 90, code = 3, length = 0.1)
arrows(10000, conf_m2[2], 10000, conf_m2[3], col = "green", angle = 90, code = 3, length = 0.1)
install.packages("tinytex")
tinytex::install_tinytex()
install.packages("rmarkdown")
install.packages("tinytex")
tinytex::install_tinytex()
install.packages("tinytex")
tinytex::install_tinytex()
install.packages("tinytex")
tinytex::install_tinytex()
df <- read.csv("synthetic_fraud_dataset.csv")
df <- read.csv("synthetic_fraud_dataset.csv")
str(df)
summary(df)
head(df)
head(sf)
sf <- read.csv("synthetic_fraud_dataset.csv")
str(df)
summary(df)
ls
getwd()
setwd("/Users/hyeonlee/Desktop")
getwd()
setwd(/Side Projects)
setwd("/Users/hyeonlee/Desktop/Side_Projects/CCFraudDetector")
getwd()
str(df)
df <- read.csv("synthetic_fraud_dataset.csv")
str(df)
summary(df)
head(df)
#Stage 2: 
colSums(is.na(df))
#Stage 3: 

table(df$Class)
prop.table(table(df$Class))
library(readr)
synthetic_fraud_dataset <- read_csv("~/Desktop/Side Projects/CCFraudDetector/synthetic_fraud_dataset.csv")
library(readr)
synthetic_fraud_dataset <- read_csv("~/Desktop/Side Projects/CCFraudDetector/synthetic_fraud_dataset.csv")
